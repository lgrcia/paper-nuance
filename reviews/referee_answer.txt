In what follows, all references correspond to lines in the revised manuscript, where changes have been highlighted in red. We thank the referee for an extremely meaningful report, we believe the paper will truly benefit from their experience on the subject.

Referee:
The selection of BLS (instead of TLS) together with simultanous "best" detrending seems odd at first glance. Why not combine both? The reason is clear however (and should be stated explicitely): Planetary transit detection around active stars is hindered almost entirely by stellar noise, not by the small change caused by planetary limb-darkening. The authors correctly note a (small) possible improvement by TLS (section 4.4) and this is fine. But the neglect of TLS-type methods should be explained early on as well.

Authors:
We agree that an early justification would be better. We added a sentence in line 374.


Referee:
I am not at all convinced by the results of the synthetic tests. It is too easy to create synthetic data that are particularly useful for a given method (and vice-versa). Specifically, the periodic GP kernel (while physically motivated) and the (multi)periodic data obviously work together well. This test does not show, however, that the method is useful for real-world photometry. I recommend the authors make this clearer in the synthetic section, also giving a stronger motivation for the real-world test.

Authors:
We made that point a bit more explicit in line 473, highlighting the "synthetic" value of these tests.

Referee:
For the real-world tests, I'm a bit confused. The text states injection experiments "from a list of 438 targets" - how many different light curves were chosen for injections? All 438 or a subset of these? How many injections were made per LC, and how many in total? I would guess 438x100, but I'm not sure. It is certainly necessary to test a large randomly chosen variety of active star light curves for such injection-retrieval experiments. This way, over-tuning and selection bias can be reduced. The section on top of page 22 should be written more clearly.

Authors:
We did use the full dataset of 438 light curves (one half-sector for each target). We made this point much clearer throughout the manuscript, in lines 482, 485, 492, 514 and 524.

Referee:
In addition, it would be useful to state an average (or median) planetary radius (in physical units, not just in SNR) of the injections to gain some insights into the "real" success of this new method: What planet discoveries can be expected? By "how much" is the new method better, e.g. as measured in planetary radius for an e.g. SNR=10 detection?

Authors:
We thank the referee for this very meaningful suggestion. Line 524 gives some first order statistics about the injected planets radii. We also added an extensive study of the kind of planets for which nuance tends to be beneficial (line 601 and figure 14). Although we give an explanation of why nuance is better at finding larger planets here, we warn the readers that this result is very specific to the studied dataset. Nonetheless, it is nice to show that nuance is at least as performant as other methods in finding planets with all kinds of radii.

Referee:
Finally, your readers are wondering: You have searched 438 (?) previously untouched light curves of active stars. Have planets been found in these real data with the new method (before or after injection)? Of course this is a paper focusing on a new tool and not planet discovery, but a note seems to be in order of either a companion paper that is being worked on (or not). That way, the community can avoid double work.

Authors:
We did not make such a search, and the community should be encouraged to do it. We make that explicit in line 614. 

Referee:
Lastly, one item is missing for section 4.4 "prospects". The "nuance" periodogram (Figure 7), just as the "BLS periodogram", suffers from an increase of SNR with larger periods P. This increase is an algorithmic artifact and can be (approximately) removed (for a clearer trend visualization see Figure 3a and section 2.2 in Hippke 2019). The underyling effect was first explained well by Ofir 2014. I think it is not strictly required for "nuance" to consider this effect, but it should be added to the list of suggested future improvements (especially because being almost trivial to implement).

Authors:
After investigation it seems that, since our periodogram is made of the actual SNR of the transit, this effect might not be an algorithmic artifact. It makes sense that the longer period model, corresponding to a unique transit when the period is large enough (See figure 7) have a higher SNR than a shorter-period signal where multiple transits with a given period are forced to fit the light curve. Ofir 2014 (section 5.1) suggests that "a progressively smaller number of points are actually required to be in-transit, and so the probability that the mean value of smaller random groups of points will be different from the global mean will increase, driving both the mean value and the scatter of the raw BLS spectrum up". Even if this point was valid in our case, an increase of the transit depth uncertainty could also lead to a smaller SNR. That being said, we acknowledge that there is a fundamental parallel in all other periodograms constructs (such as the Lomb-Scargle). For now, as we do not have a theoretical justification to detrend the SNR periodogram, we prefer not to mention this point and defer this addition to an eventual upgrade of the nuance package and documentation.

Referee:
Two real-world examples are dicussed: TOI-540 (Figure 14) and TIC 1019692 (Figure 10). Are these "typical examples" or why were they chosen?

Authors:
These example are indeed typical, which we highlight in lines 578 and 623.

Other changes:
- An extra past affiliation has been added to the author list (line 5)
- A mistake about the second-best method has been fixed in the caption of Figure 12.
- Fix "Nonetheless" line 661
- Added "shorter-timescale" line 801
- Addition to the acknowledgements in line 858


----

Data Editor:
Per our software policy [1], we recommend that authors with living code in a GitHub repository place a "frozen" version on Zenodo (or other 3rd party repository that issues DOIs) [2,3] and then cite the deposited version in the article [4] and it in the reference list

Authors:
A Zenodo record was created for version 0.6.0 and cited in the paper (line 332 and 884).

Data Editor:
Note that the astropy collaboration requests that all 3 of their articles be cited

Authors:
All references for astropy were added in line 882.


Again, many thanks to both the referee and data editor!